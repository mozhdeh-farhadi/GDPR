{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDPR Analysis Main Notebook \n",
    "\n",
    "**Goal** \n",
    "The goal of this notebook is to demonstrate usage of this codebase as well as ultimately assembling the main \"dataframe\" of this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats \n",
    "import json\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **note** about the code: The data for this study is partitioned into two sets, _EU_ and _Global_, based on the location of the domain. Most of the analysis is done by sequentially applying a process to each set before moving on, and the code will reflect as much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_df = pd.read_csv('./eu_dataframe.csv')\n",
    "g_df = pd.read_csv('./global_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_dict = {}\n",
    "for i in eu_df.itertuples():\n",
    "    eu_dict[i.pid] = {'pid': i.pid, 'pre_ts':i.pre_ts, 'post_ts':i.post_ts}\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = {}\n",
    "for i in g_df.itertuples():\n",
    "    g_dict[i.pid] = {'pid': i.pid, 'pre_ts':i.pre_ts, 'post_ts':i.post_ts}\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_html_dir = Path('./raw_html/EU/')\n",
    "for path in eu_html_dir.iterdir(): \n",
    "    if not path.is_dir(): continue \n",
    "    try: \n",
    "        pid = int(str(path).split('/')[-1])\n",
    "        if eu_dict.get(pid) is not None: eu_dict[pid]['raw_html_dir'] = str(path)\n",
    "    except: \n",
    "        continue\n",
    "    continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_html_dir = Path('./raw_html/Global/')\n",
    "for path in g_html_dir.iterdir(): \n",
    "    if not path.is_dir(): continue \n",
    "    try: \n",
    "        pid = int(str(path).split('/')[-1])\n",
    "        if g_dict.get(pid) is not None: g_dict[pid]['raw_html_dir'] = str(path)\n",
    "    except: \n",
    "        continue\n",
    "    continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eu_text_dir = Path('./text_data/EU/')\n",
    "for path in eu_text_dir.iterdir(): \n",
    "    if not path.is_dir(): continue \n",
    "    try: \n",
    "        pid = int(str(path).split('/')[-1])\n",
    "        if eu_dict.get(pid) is not None: eu_dict[pid]['text_dir'] = str(path)\n",
    "    except: \n",
    "        continue\n",
    "    continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_text_dir = Path('./text_data/Global/')\n",
    "for path in g_text_dir.iterdir(): \n",
    "    if not path.is_dir(): continue \n",
    "    try: \n",
    "        pid = int(str(path).split('/')[-1])\n",
    "        if g_dict.get(pid) is not None: g_dict[pid]['text_dir'] = str(path)\n",
    "    except: \n",
    "        continue\n",
    "    continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polisis Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_polisis_pre_dir = Path('./polisis_outputs/EU/pre_data/')\n",
    "for f in eu_polisis_pre_dir.iterdir(): \n",
    "    if not str(f).endswith('.json'): continue\n",
    "    try: \n",
    "        pid = int(str(f).split('/')[-1].replace('.json', ''))\n",
    "        if eu_dict.get(pid) is not None: eu_dict[pid]['pre_polisis_output'] = str(f)\n",
    "    except:\n",
    "        continue\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_polisis_post_dir = Path('./polisis_outputs/EU/post_data/')\n",
    "for f in eu_polisis_post_dir.iterdir(): \n",
    "    if not str(f).endswith('.json'): continue\n",
    "    try: \n",
    "        pid = int(str(f).split('/')[-1].replace('.json', ''))\n",
    "        if eu_dict.get(pid) is not None: eu_dict[pid]['post_polisis_output'] = str(f)\n",
    "    except:\n",
    "        continue\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_polisis_pre_dir = Path('./polisis_outputs/Global/pre_data/')\n",
    "for f in g_polisis_pre_dir.iterdir(): \n",
    "    if not str(f).endswith('.json'): continue\n",
    "    try: \n",
    "        pid = int(str(f).split('/')[-1].replace('.json', ''))\n",
    "        if g_dict.get(pid) is not None: g_dict[pid]['pre_polisis_output'] = str(f)\n",
    "    except:\n",
    "        continue\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_polisis_post_dir = Path('./polisis_outputs/Global/post_data/')\n",
    "for f in g_polisis_post_dir.iterdir(): \n",
    "    if not str(f).endswith('.json'): continue\n",
    "    try: \n",
    "        pid = int(str(f).split('/')[-1].replace('.json', ''))\n",
    "        if g_dict.get(pid) is not None: g_dict[pid]['post_polisis_output'] = str(f)\n",
    "    except:\n",
    "        continue\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.passive_voice_index import PassiveVoice # requires spacy \n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pv = PassiveVoice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_readability_scores(path, pv):\n",
    "    txt = ''\n",
    "    with open(path) as fi: \n",
    "        txt = fi.read()\n",
    "    local = {}\n",
    "    try:\n",
    "        local['syllables'] = textstat.syllable_count(txt)\n",
    "    except:\n",
    "        local['syllables'] = 0\n",
    "    try:\n",
    "        local['words'] = textstat.lexicon_count(txt, True)\n",
    "    except:\n",
    "        local['words'] = 0\n",
    "    try:\n",
    "        local['sentences'] = textstat.sentence_count(txt)\n",
    "    except:\n",
    "        local['sentences'] = 0    \n",
    "    if local.get('words') > 0: \n",
    "        local['syllables_per_word'] = local.get('syllables') / local.get('words')\n",
    "    else: \n",
    "        local['syllables_per_word'] = 0\n",
    "    if local.get('sentences') > 0: \n",
    "        local['words_per_sentence'] = local.get('words') / local.get('sentences')\n",
    "    else: \n",
    "        local['words_per_sentence'] = 0\n",
    "    try: \n",
    "        local['passive_voice'] = pv.index(txt)[0]\n",
    "    except: \n",
    "        local['passive_voice'] = 'NA'\n",
    "    return local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eu_pre_scores_dict = {}\n",
    "eu_post_scores_dict = {}\n",
    "for obj in eu_dict.values():\n",
    "    try: \n",
    "        pre_txt_fname = obj.get('text_dir') + '/%s.txt' % str(obj.get('pre_ts'))\n",
    "        eu_pre_scores_dict[obj.get('pid')] = calculate_readability_scores(pre_txt_fname, pv)\n",
    "        post_txt_fname = obj.get('text_dir') + '/%s.txt' % str(obj.get('post_ts'))\n",
    "        eu_post_scores_dict[obj.get('pid')] = calculate_readability_scores(post_txt_fname, pv)\n",
    "    except Exception as e: \n",
    "        continue\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_pre_scores_dict = {}\n",
    "g_post_scores_dict = {}\n",
    "for obj in g_dict.values():\n",
    "    try: \n",
    "        pre_txt_fname = obj.get('text_dir') + '/%s.txt' % str(obj.get('pre_ts'))\n",
    "        g_pre_scores_dict[obj.get('pid')] = calculate_readability_scores(pre_txt_fname, pv)\n",
    "        post_txt_fname = obj.get('text_dir') + '/%s.txt' % str(obj.get('post_ts'))\n",
    "        g_post_scores_dict[obj.get('pid')] = calculate_readability_scores(post_txt_fname, pv)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        continue\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_score_datarows = []\n",
    "for pid in eu_post_scores_dict:\n",
    "    pre_data = eu_pre_scores_dict.get(pid)\n",
    "    post_data = eu_post_scores_dict.get(pid)\n",
    "    local_object = {'pid':pid}\n",
    "    for i in eu_pre_scores_dict[pid].items(): local_object['pre_' + i[0]] = i[1]\n",
    "    for i in eu_post_scores_dict[pid].items(): local_object['post_' + i[0]] = i[1]\n",
    "    eu_score_datarows.append(local_object)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_score_datarows = []\n",
    "for pid in g_post_scores_dict:\n",
    "    pre_data = g_pre_scores_dict.get(pid)\n",
    "    post_data = g_post_scores_dict.get(pid)\n",
    "    local_object = {'pid':pid}\n",
    "    for i in g_pre_scores_dict[pid].items(): local_object['pre_' + i[0]] = i[1]\n",
    "    for i in g_post_scores_dict[pid].items(): local_object['post_' + i[0]] = i[1]\n",
    "    g_score_datarows.append(local_object)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_score_df = pd.DataFrame.from_dict(eu_score_datarows)\n",
    "g_score_df = pd.DataFrame.from_dict(g_score_datarows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    " 'syllables',\n",
    " 'words',\n",
    " 'sentences',\n",
    " 'syllables_per_word',\n",
    " 'words_per_sentence',\n",
    " 'passive_voice'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU Text Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_agg_df = eu_score_df[['pre_' + metrics[0], 'post_' + metrics[0]]].describe() \n",
    "for m in metrics[1:]: \n",
    "    new_df = eu_score_df[['pre_' + m, 'post_' + m]].describe() \n",
    "    eu_agg_df = pd.concat([eu_agg_df, new_df], sort=False, axis=1)\n",
    "eu_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Text Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_agg_df = g_score_df[['pre_' + metrics[0], 'post_' + metrics[0]]].describe() \n",
    "for m in metrics[1:]: \n",
    "    new_df = g_score_df[['pre_' + m, 'post_' + m]].describe() \n",
    "    g_agg_df = pd.concat([g_agg_df, new_df], sort=False, axis=1)\n",
    "g_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU Wilcoxon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_wilcoxon = [ {'metric':m, 'statistic':stats.wilcoxon(eu_score_df['pre_' + m], eu_score_df['post_' + m])[0],'p-val':stats.wilcoxon(eu_score_df['pre_' + m], eu_score_df['post_' + m])[1]}  for m in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_wil_df = pd.DataFrame.from_dict(eu_wilcoxon)\n",
    "print(eu_wil_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Wilcoxon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_wilcoxon = [ {'metric':m, 'statistic':stats.wilcoxon(g_score_df['pre_' + m], g_score_df['post_' + m])[0],'p-val':stats.wilcoxon(g_score_df['pre_' + m], g_score_df['post_' + m])[1]}  for m in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_wil_df = pd.DataFrame.from_dict(g_wilcoxon)\n",
    "print(g_wil_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query data is organized in the following structure: \n",
    "```\n",
    "/queries\n",
    "    /coverage > /location.json > pid > source: [strings] \n",
    "    /compliance > /location.json > pid > query > source: {count: int}\n",
    "    /specificity > /location.json > pid > query > source: {S: int, S_a: int}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_categories = [\n",
    "    'policy_change', 'first_party_collection_use', 'user_choice_control', 'international_and_specific_audiences',\n",
    "     'user_access_edit_and_deletion', 'data_retention', 'data_security', 'privacy_contact_information',\n",
    "     'third_party_sharing_collection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_coverage_data = json.load(open('./polisis_queries/coverage/EU.json'))\n",
    "eu_coverage_by_category = {} # scoreboard\n",
    "for pid in eu_coverage_data:\n",
    "    for src in ['archive', 'current']: # a.k.a pre/post\n",
    "        for category in all_categories: \n",
    "            if eu_coverage_by_category.get(category) is None: eu_coverage_by_category[category] = {'archive': {'case':0, 'total':0}, 'current':{'case':0, 'total':0}}\n",
    "            if category in eu_coverage_data.get(pid).get(src): eu_coverage_by_category[category][src]['case'] += 1\n",
    "            eu_coverage_by_category[category][src]['total'] += 1\n",
    "\n",
    "            continue # categories\n",
    "        continue # src    \n",
    "    continue # pids \n",
    "    \n",
    "eu_coverage_lod = []\n",
    "for cat in eu_coverage_by_category:\n",
    "    pre_case = eu_coverage_by_category.get(cat).get('archive').get('case')\n",
    "    pre_total = eu_coverage_by_category.get(cat).get('archive').get('total')\n",
    "    pre_pct = 100*pre_case/pre_total\n",
    "    \n",
    "    post_case = eu_coverage_by_category.get(cat).get('current').get('case')\n",
    "    post_total = eu_coverage_by_category.get(cat).get('current').get('total')\n",
    "    post_pct = 100*post_case/post_total\n",
    "    \n",
    "    eu_coverage_lod.append({'category':cat, 'source':'archive', 'num_covered_per': pre_pct })\n",
    "    eu_coverage_lod.append({'category':cat, 'source':'current', 'num_covered_per': post_pct})\n",
    "    continue \n",
    "\n",
    "eu_coverage_df = pd.DataFrame.from_dict(eu_coverage_lod)\n",
    "eu_coverage_df = eu_coverage_df.groupby(['category', 'source'], sort=False).sum().reset_index()\n",
    "eu_coverage_df.sort_values(['category'], ascending=[True], inplace=True)\n",
    "eu_coverage_df.sort_values(['source'], ascending=[True], inplace=True)\n",
    "eu_coverage_df.sort_values(['category'], ascending=[True], inplace=True)\n",
    "eu_coverage_plot = sns.catplot(x=\"category\", y='num_covered_per', hue=\"source\", data=eu_coverage_df, kind=\"bar\", aspect=80 / 40, orient='v', ci=None, palette=['#bae4bc', '#43a2ca'], legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_coverage_data = json.load(open('./polisis_queries/coverage/Global.json'))\n",
    "g_coverage_by_category = {} # scoreboard\n",
    "for pid in g_coverage_data:\n",
    "    for src in ['archive', 'current']: # a.k.a pre/post\n",
    "        for category in all_categories: \n",
    "            if g_coverage_by_category.get(category) is None: g_coverage_by_category[category] = {'archive': {'case':0, 'total':0}, 'current':{'case':0, 'total':0}}\n",
    "            if category in g_coverage_data.get(pid).get(src): g_coverage_by_category[category][src]['case'] += 1\n",
    "            g_coverage_by_category[category][src]['total'] += 1\n",
    "\n",
    "            continue # categories\n",
    "        continue # src    \n",
    "    continue # pids \n",
    "    \n",
    "g_coverage_lod = []\n",
    "for cat in g_coverage_by_category:\n",
    "    pre_case = g_coverage_by_category.get(cat).get('archive').get('case')\n",
    "    pre_total = g_coverage_by_category.get(cat).get('archive').get('total')\n",
    "    pre_pct = 100*pre_case/pre_total\n",
    "    \n",
    "    post_case = g_coverage_by_category.get(cat).get('current').get('case')\n",
    "    post_total = g_coverage_by_category.get(cat).get('current').get('total')\n",
    "    post_pct = 100*post_case/post_total\n",
    "    \n",
    "    g_coverage_lod.append({'category':cat, 'source':'archive', 'num_covered_per': pre_pct })\n",
    "    g_coverage_lod.append({'category':cat, 'source':'current', 'num_covered_per': post_pct})\n",
    "    continue \n",
    "\n",
    "g_coverage_df = pd.DataFrame.from_dict(g_coverage_lod)\n",
    "g_coverage_df = g_coverage_df.groupby(['category', 'source'], sort=False).sum().reset_index()\n",
    "g_coverage_df.sort_values(['category'], ascending=[True], inplace=True)\n",
    "g_coverage_df.sort_values(['source'], ascending=[True], inplace=True)\n",
    "g_coverage_df.sort_values(['category'], ascending=[True], inplace=True)\n",
    "g_coverage_plot = sns.catplot(x=\"category\", y='num_covered_per', hue=\"source\", data=g_coverage_df, kind=\"bar\", aspect=80 / 40, orient='v', ci=None, palette=['#bae4bc', '#43a2ca'], legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compliance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eu_compliance_data = json.load(open('./polisis_queries/compliance/EU.json'))\n",
    "\n",
    "eu_compliance_by_query = {}\n",
    "for pid in eu_compliance_data:\n",
    "    queryset = eu_compliance_data.get(pid).get('queries').get('rights')\n",
    "    for q in queryset:\n",
    "        if q == 'retention_period_of_personal_data': continue\n",
    "        # Init if needed\n",
    "        if eu_compliance_by_query.get(q) is None: eu_compliance_by_query[q] = {'fully':0, 'improved':0, 'never':0, 'worse':0}\n",
    "        # Set flags for compliance\n",
    "        is_pre_comp = queryset.get(q).get('archive').get('count') > 0\n",
    "        is_post_comp = queryset.get(q).get('current').get('count') > 0 \n",
    "        \n",
    "        # Compare pre v. post\n",
    "        result = (\n",
    "            'fully' if is_pre_comp and is_post_comp else \n",
    "            'improved' if is_post_comp and not is_pre_comp else \n",
    "            'worse' if is_pre_comp and not is_post_comp else \n",
    "            'never'\n",
    "        )\n",
    "        \n",
    "        # Update\n",
    "        eu_compliance_by_query[q][result] += 1\n",
    "        \n",
    "        \n",
    "        continue # queries\n",
    "    continue # pids\n",
    "\n",
    "eu_compliance_datarows = []\n",
    "for query in eu_compliance_by_query:\n",
    "    total = sum([eu_compliance_by_query.get(query).get(result) for result in ['fully', 'improved', 'worse', 'never']])\n",
    "    eu_compliance_datarows.append({\n",
    "        'query':query, \n",
    "        'fully':eu_compliance_by_query.get(query).get('fully')/total,\n",
    "        'improved':eu_compliance_by_query.get(query).get('improved')/total,\n",
    "        'worse':eu_compliance_by_query.get(query).get('worse')/total,\n",
    "        'never':eu_compliance_by_query.get(query).get('never')/total,\n",
    "    })\n",
    "    \n",
    "\n",
    "eu_compl_df = pd.DataFrame.from_dict(eu_compliance_datarows)\n",
    "eu_compl_df = eu_compl_df.groupby('query', sort=False).agg(['sum'])\n",
    "eu_plotter = eu_compl_df.div(eu_compl_df.sum(axis=1), axis=0).multiply(100)\n",
    "\n",
    "ax = eu_plotter.plot(kind='barh', stacked=True, colormap='tab20c', sort_columns=True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylabel('ICO Query ID')\n",
    "ax.set_xlabel('% Policies')\n",
    "legend_list = ['Fully','Improved', 'Never', 'Worse']\n",
    "plt.legend(legend_list, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "g_compliance_data = json.load(open('./polisis_queries/compliance/Global.json'))\n",
    "\n",
    "g_compliance_by_query = {}\n",
    "for pid in g_compliance_data:\n",
    "    queryset = g_compliance_data.get(pid).get('queries').get('rights')\n",
    "    for q in queryset:\n",
    "        if q == 'retention_period_of_personal_data': continue\n",
    "        # Init if needed\n",
    "        if g_compliance_by_query.get(q) is None: g_compliance_by_query[q] = {'fully':0, 'improved':0, 'never':0, 'worse':0}\n",
    "        # Set flags for compliance\n",
    "        is_pre_comp = queryset.get(q).get('archive').get('count') > 0\n",
    "        is_post_comp = queryset.get(q).get('current').get('count') > 0 \n",
    "        \n",
    "        # Compare pre v. post\n",
    "        result = (\n",
    "            'fully' if is_pre_comp and is_post_comp else \n",
    "            'improved' if is_post_comp and not is_pre_comp else \n",
    "            'worse' if is_pre_comp and not is_post_comp else \n",
    "            'never'\n",
    "        )\n",
    "        \n",
    "        # Update\n",
    "        g_compliance_by_query[q][result] += 1\n",
    "        \n",
    "        \n",
    "        continue # queries\n",
    "    continue # pids\n",
    "\n",
    "g_compliance_datarows = []\n",
    "for query in g_compliance_by_query:\n",
    "    total = sum([g_compliance_by_query.get(query).get(result) for result in ['fully', 'improved', 'worse', 'never']])\n",
    "    g_compliance_datarows.append({\n",
    "        'query':query, \n",
    "        'fully':g_compliance_by_query.get(query).get('fully')/total,\n",
    "        'improved':g_compliance_by_query.get(query).get('improved')/total,\n",
    "        'worse':g_compliance_by_query.get(query).get('worse')/total,\n",
    "        'never':g_compliance_by_query.get(query).get('never')/total,\n",
    "    })\n",
    "    \n",
    "\n",
    "g_compl_df = pd.DataFrame.from_dict(g_compliance_datarows)\n",
    "g_compl_df = g_compl_df.groupby('query', sort=False).agg(['sum'])\n",
    "g_plotter = g_compl_df.div(g_compl_df.sum(axis=1), axis=0).multiply(100)\n",
    "\n",
    "ax = g_plotter.plot(kind='barh', stacked=True, colormap='tab20c', sort_columns=True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylabel('ICO Query ID')\n",
    "ax.set_xlabel('% Policies')\n",
    "legend_list = ['Fully','Improved', 'Never', 'Worse']\n",
    "plt.legend(legend_list, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
